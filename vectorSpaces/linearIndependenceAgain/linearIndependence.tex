\documentclass{ximera}

\input{../../preamble.tex}

\title{Linear Independence and Spanning Sets}

\begin{document}
\begin{abstract}
  We extend our definition of linear independence to the setting of a general vector space.
\end{abstract}
\maketitle

A vector space is defined as a set with two operations, meeting ten
properties (\ref{definition:VS}).  Just as the definition of span of a
set of vectors only required knowing how to add vectors and how to
multiply vectors by scalars, so it is with linear independence.  A
definition of a linearly independent set of vectors in an arbitrary
vector space only requires knowing how to form linear combinations and
equating these with the zero vector.  Since every vector space must
have a zero vector (\ref{property:Z}), we always have a zero vector at
our disposal.

In these activities we will also put a twist on the notion of the span
of a set of vectors.  Rather than beginning with a set of vectors and
creating a subspace that is the span, we will instead begin with a
subspace and look for a set of vectors whose span equals the subspace.

The combination of linear independence and spanning will be very
important going forward.

Our previous definition of linear independence (\ref{definition:LICV})
employed a relation of linear dependence that was a linear combination
on one side of an equality and a zero vector on the other side.  As a
linear combination in a vector space (\ref{definition:LC}) depends
only on vector addition and scalar multiplication, and every vector
space must have a zero vector (\ref{property:Z}), we can extend our
definition of linear independence from the setting of $\complex{m}$ to
the setting of a general vector space $V$ with almost no changes.

\begin{definition}[Relation of Linear Dependence]
  Suppose that $V$ is a vector space.  Given a set of vectors
  $S=\set{\vectorlist{u}{n}}$, an equation of the form
  \[
    \lincombo{\alpha}{u}{n}=\zerovector
  \]
  is a \dfn{relation of linear dependence} on $S$.  If this equation
  is formed in a trivial fashion, i.e., $\alpha_i=0$, $1\leq i\leq n$,
  then we say it is a \dfn{trivial relation of linear dependence} on
  $S$.
\end{definition}

\begin{definition}[Linear Independence]
  Suppose that $V$ is a vector space.  The set of vectors
  $S=\set{\vectorlist{u}{n}}$ from $V$ is \dfn{linearly dependent} if
  there is a relation of linear dependence on $S$ that is not trivial.
  In the case where the \textit{only} relation of linear dependence on
  $S$ is the trivial one, then $S$ is a \dfn{linearly independent} set
  of vectors.
\end{definition}

Notice the emphasis on the word ``only.''  This might remind you of
the definition of a nonsingular matrix, where if the matrix is
employed as the coefficient matrix of a homogeneous system then the
\textit{only} solution is the \textit{trivial} one.

\begin{example}[Linear independence in $P_4$]
  In the vector space of polynomials with degree 4 or less, $P_4$
  (\ref{example:VSP}) consider the set $S$ below
  \begin{align*}
    \set{
    2x^4+3x^3+2x^2-x+10,\,
    -x^4-2x^3+x^2+5x-8,\,
    2x^4+x^3+10x^2+17x-2
    }
  \end{align*}
  
  Is this set of vectors linearly independent or dependent?  Consider
  that
  \begin{align*}
    &3\left(2x^4+3x^3+2x^2-x+10\right)
      +4\left(-x^4-2x^3+x^2+5x-8\right)\\
    &\quad +(-1)\left(2x^4+x^3+10x^2+17x-2\right)
      =0x^4+0x^3+0x^2+0x+0=\zerovector
  \end{align*}
  This is a nontrivial relation of linear dependence
  (\ref{definition:RLD}) on the set $S$ and so convinces us that $S$
  is linearly dependent (\ref{definition:LI}).

  Now, I hear you say, ``Where did \textit{those} scalars come from?''
  Do not worry about that right now, just be sure you understand why
  the above explanation is sufficient to prove that $S$ is linearly
  dependent.  The remainder of the example will demonstrate how we
  might find these scalars if they had not been provided so readily.

  Let us look at another set of vectors (polynomials) from $P_4$.  Let
  \begin{align*}
    T&=\left\{
       3x^4-2x^3+4x^2+6x-1,\,
       -3x^4+1x^3+0x^2+4x+2,\right.\\
     &\quad \left.4x^4+5x^3-2x^2+3x+1,\,
       2x^4-7x^3+4x^2+2x+1\right\}
  \end{align*}

  Suppose we have a relation of linear dependence on this set,
  \begin{align*}
    \zerovector&=0x^4+0x^3+0x^2+0x+0\\
               &=\alpha_1\left(3x^4-2x^3+4x^2+6x-1\right)+\alpha_2\left(-3x^4+1x^3+0x^2+4x+2\right)\\
               &\quad +\alpha_3\left(4x^4+5x^3-2x^2+3x+1\right)+\alpha_4\left(2x^4-7x^3+4x^2+2x+1\right)
  \end{align*}

  Using our definitions of vector addition and scalar multiplication in $P_4$ (\ref{example:VSP}), we arrive at,
  \begin{align*}
    &0x^4+0x^3+0x^2+0x+0=\\
    &\quad\left(3\alpha_1-3\alpha_2+4\alpha_3+2\alpha_4\right)x^4 + \left(-2\alpha_1+\alpha_2+5\alpha_3-7\alpha_4\right)x^3 +\ \\
    &\quad\left(4\alpha_1-2\alpha_3+4\alpha_4\right)x^2+\left(6\alpha_1+4\alpha_2+3\alpha_3+2\alpha_4\right)x + \left(-\alpha_1+2\alpha_2+\alpha_3+\alpha_4\right)
  \end{align*}

  Equating coefficients, we arrive at the homogeneous system of equations,
  \begin{align*}
    3\alpha_1-3\alpha_2+4\alpha_3+2\alpha_4&=0\\
    -2\alpha_1+\alpha_2+5\alpha_3-7\alpha_4&=0\\
    4\alpha_1-2\alpha_3+4\alpha_4&=0\\
    6\alpha_1+4\alpha_2+3\alpha_3+2\alpha_4&=0\\
    -\alpha_1+2\alpha_2+\alpha_3+\alpha_4&=0
  \end{align*}

  We form the coefficient matrix of this homogeneous system of equations and row-reduce to find
  \[
    \begin{bmatrix}
      \leading{1} & 0 & 0 & 0\\
      0 & \leading{1} & 0 & 0\\
      0 & 0 & \leading{1} & 0\\
      0 & 0 & 0 & \leading{1}\\
      0 & 0 & 0 & 0
    \end{bmatrix}
  \]

  We expected the system to be consistent (\ref{theorem:HSC}) and so
  can compute $n-r=4-4=0$ and \ref{theorem:CSRN} tells us that the
  solution is unique.  Since this is a homogeneous system, this unique
  solution is the trivial solution (\ref{definition:TSHSE}),
  $\alpha_1=0$, $\alpha_2=0$, $\alpha_3=0$, $\alpha_4=0$.  So by
  \ref{definition:LI} the set $T$ is linearly \wordChoice{\choice{dependent}\choice[correct]{independent}}.

  A few observations.  If we had discovered infinitely many solutions,
  then we could have used one of the nontrivial solutions to provide a
  linear combination in the manner we used to show that $S$ was
  linearly dependent.  It is important to realize that it is not
  interesting that we can create a relation of linear dependence with
  zero scalars---we can \textit{always} do that---but for $T$, this is
  the \textit{only} way to create a relation of linear dependence.  It
  was no accident that we arrived at a homogeneous system of equations
  in this example, it is related to our use of the zero vector in
  defining a relation of linear dependence.

  It is easy to present a convincing statement that a set is linearly
  dependent (just exhibit a nontrivial relation of linear dependence)
  but a convincing statement of linear independence requires
  demonstrating that there is no relation of linear dependence other
  than the trivial one.  Notice how we relied on theorems from
  \ref{chapter:SLE} to provide this demonstration.  Whew!  There is a
  lot going on in this example.
\end{example}

\begin{example}[Linear independence in $M_{32}$]

  Consider the two sets of vectors $R$ and $S$ from the vector space of all $3\times 2$ matrices, $M_{32}$ (\ref{example:VSM})
  \begin{align*}
    R&=\set{
       \begin{bmatrix}
         3 & -1\\1 & 4\\6 & -6
       \end{bmatrix},\,
                            \begin{bmatrix}
                              -2 & 3\\1 & -3\\-2 & -6
                            \end{bmatrix},\,
                                                   \begin{bmatrix}
                                                     6 & -6\\-1 & 0\\7 & -9
                                                   \end{bmatrix},\,
                                                                         \begin{bmatrix}
                                                                           7 & 9\\-4 & -5\\2 & 5
                                                                         \end{bmatrix}
                                                                                               }\\
    S&=\set{
       \begin{bmatrix}
         2 & 0\\ 1 & -1\\ 1 & 3
       \end{bmatrix},\,
                              \begin{bmatrix}
                                -4 & 0\\ -2 & 2\\ -2 & -6
                              \end{bmatrix},\,
                                                       \begin{bmatrix}
                                                         1 & 1\\ -2 & 1\\ 2 & 4
                                                       \end{bmatrix},\,
                                                                              \begin{bmatrix}
                                                                                -5 & 3\\ -10 & 7\\ 2 & 0
                                                                              \end{bmatrix}
                                                                                                       }
  \end{align*}
  
  One set is linearly independent, the other is not.  Which is which?

  Let us examine $R$ first.  Build a generic relation of linear
  dependence (\ref{definition:RLD}),
  \[
    \alpha_1\begin{bmatrix}
      3 & -1\\1 & 4\\6 & -6
    \end{bmatrix}+
    \alpha_2\begin{bmatrix}
      -2 & 3\\1 & -3\\-2 & -6
    \end{bmatrix}+
    \alpha_3\begin{bmatrix}
      6 & -6\\-1 & 0\\7 & -9
    \end{bmatrix}+
    \alpha_4\begin{bmatrix}
      7 & 9\\-4 & -5\\2 & 5
    \end{bmatrix}=
    \zerovector
  \]

  Massaging the left-hand side with our definitions of vector addition and scalar multiplication in $M_{32}$ (\ref{example:VSM}) we obtain,
  \[
    \begin{bmatrix}
      3\alpha_1-2\alpha_2+6\alpha_3+7\alpha_4 &
      -\alpha_1+3\alpha_2-6\alpha_3+9\alpha_4 \\
      \alpha_1+\alpha_2-\alpha_3-4\alpha_4 &
      4\alpha_1-3\alpha_2+            -5\alpha_4 \\
      6\alpha_1-2\alpha_2+7\alpha_3+2\alpha_4 &
      -6\alpha_1-6\alpha_2-9\alpha_3+5\alpha_4
    \end{bmatrix}
    =\begin{bmatrix}
      0&0\\0&0\\0&0
    \end{bmatrix}
  \]
  
  
  Using our definition of matrix equality (\ref{definition:ME}) to equate entries we get the homogeneous system of six equations in four variables,
  \begin{align*}
    3\alpha_1-2\alpha_2+6\alpha_3+7\alpha_4&=0\\
    -\alpha_1+3\alpha_2-6\alpha_3+9\alpha_4&=0\\
    \alpha_1+\alpha_2-\alpha_3-4\alpha_4&=0\\
    4\alpha_1-3\alpha_2+            -5\alpha_4&=0\\
    6\alpha_1-2\alpha_2+7\alpha_3+2\alpha_4&=0\\
    -6\alpha_1-6\alpha_2-9\alpha_3+5\alpha_4&=0
  \end{align*}
  

  Form the coefficient matrix of this homogeneous system and row-reduce to obtain
  \[
    \begin{bmatrix}
      \leading{1} & 0 & 0 & 0\\
      0 & \leading{1} & 0 & 0\\
      0 & 0 & \leading{1} & 0\\
      0 & 0 & 0 & \leading{1}\\
      0 & 0 & 0 & 0\\
      0 & 0 & 0 & 0
    \end{bmatrix}
  \]
  
  Analyzing this matrix we are led to conclude that $\alpha_1=0$,
  $\alpha_2=0$, $\alpha_3=0$, $\alpha_4=0$.  This means there is
  \textit{only} a trivial relation of linear dependence on the vectors
  of $R$ and so we call $R$ a linearly independent set
  (\ref{definition:LI}).

  What about $S$?  Let us see if we can find a nontrivial relation of
  linear dependence on $S$.  We will begin as with $R$, by
  constructing a relation of linear dependence (\ref{definition:RLD})
  with unknown scalars,
  \[
    \alpha_1\begin{bmatrix}
      2 & 0\\ 1 & -1\\ 1 & 3
    \end{bmatrix}+
    \alpha_2\begin{bmatrix}
      -4 & 0\\ -2 & 2\\ -2 & -6
    \end{bmatrix}+
    \alpha_3\begin{bmatrix}
      1 & 1\\ -2 & 1\\ 2 & 4
    \end{bmatrix}+
    \alpha_4\begin{bmatrix}
      -5 & 3\\ -10 & 7\\ 2 & 0
    \end{bmatrix}=
    \zerovector
  \]
  
  Massaging the left-hand side with our definitions of vector addition and scalar multiplication in $M_{32}$ (\ref{example:VSM}) we obtain,
  \[
    \begin{bmatrix}
      2\alpha_1-4\alpha_2+\alpha_3-5\alpha_4&
      \alpha_3+3\alpha_4\\
      \alpha_1-2\alpha_2-2\alpha_3-10\alpha_4&
      -\alpha_1+2\alpha_2+\alpha_3+7\alpha_4\\
      \alpha_1-2\alpha_2+2\alpha_3+2\alpha_4&
      3\alpha_1-6\alpha_2+4\alpha_3
    \end{bmatrix}
    =\begin{bmatrix}
      0&0\\0&0\\0&0
    \end{bmatrix}
  \]

  Using our definition of matrix equality (\ref{definition:ME}) to equate entries we get the homogeneous system of six equations in four variables,
  \begin{align*}
    2\alpha_1-4\alpha_2+\alpha_3-5\alpha_4&=0\\
    \alpha_3+3\alpha_4&=0\\
    \alpha_1-2\alpha_2-2\alpha_3-10\alpha_4&=0\\
    -\alpha_1+2\alpha_2+\alpha_3+7\alpha_4&=0\\
    \alpha_1-2\alpha_2+2\alpha_3+2\alpha_4&=0\\
    3\alpha_1-6\alpha_2+4\alpha_3         &=0
  \end{align*}

  Form the coefficient matrix of this homogeneous system and row-reduce to obtain
  \[
    \begin{bmatrix}
      \leading{1} & -2 & 0 & -4\\
      0 & 0 & \leading{1} & 3\\
      0 & 0 & 0 & 0\\
      0 & 0 & 0 & 0\\
      0 & 0 & 0 & 0\\
      0 & 0 & 0 & 0
    \end{bmatrix}
  \]

  Analyzing this we see that the system is consistent (we expected
  this since the system is homogeneous, \ref{theorem:HSC}) and has
  $n-r=4-2=2$ free variables, 
  \begin{multipleChoice}
    \choice[correct]{namely $\alpha_2$ and $\alpha_4$.}
    \choice{namely $\alpha_1$ and $\alpha_3$.}
  \end{multipleChoice}
  This means there are infinitely many solutions, and in particular,
  we can find a nontrivial solution, so long as we do not pick all of
  our free variables to be zero.  The mere presence of a nontrivial
  solution for these scalars is enough to conclude that $S$ is a
  linearly dependent set (\ref{definition:LI}).  But let us go ahead
  and explicitly construct a nontrivial relation of linear dependence.

  Choose $\alpha_2=1$ and $\alpha_4=-1$.  There is nothing special
  about this choice, there are infinitely many possibilities, some
  ``easier'' than this one, just avoid picking both variables to be
  zero.  (Why not?) Then we find the dependent variables to have
  values $\alpha_1=\answer{-2}$ and $\alpha_3=3$.  So the relation of
  linear dependence,
  \[
    (-2)\begin{bmatrix}
      2 & 0\\ 1 & -1\\ 1 & 3
    \end{bmatrix}+
    (1)\begin{bmatrix}
      -4 & 0\\ -2 & 2\\ -2 & -6
    \end{bmatrix}+
    (3)\begin{bmatrix}
      1 & 1\\ -2 & 1\\ 2 & 4
    \end{bmatrix}+
    (-1)\begin{bmatrix}
      -5 & 3\\ -10 & 7\\ 2 & 0
    \end{bmatrix}
    =
    \begin{bmatrix}
      0&0\\0&0\\0&0
    \end{bmatrix}
  \]
  is an iron-clad demonstration that $S$ is linearly dependent.  Can you construct another such demonstration?
\end{example}

\end{document}
