\documentclass{ximera}

\input{../../preamble.tex}

\title{Linear Combinations}

\begin{document}
\begin{abstract}
  A linear combination takes scalars and vectors and combines them
  using scalar multiplication and vector addition to creates a single
  brand-new vector.
\end{abstract}
\maketitle

We defined vector addition and scalar multiplication.  These two
operations combine nicely to give us a construction known as a
\dfn{linear combination}, a construct that we will work with
throughout this course.

\begin{definition}[Linear Combination of Column Vectors]
  Given $n$ vectors $\vectorlist{u}{n}$ from $\complex{m}$ and $n$
  scalars $\alpha_1,\,\alpha_2,\,\alpha_3,\,\ldots,\,\alpha_n$, their
  \dfn{linear combination} is the vector
  \[
    \lincombo{\alpha}{u}{n}
  \]
\end{definition}

So this definition takes an equal number of scalars and vectors,
combines them using our two new operations (scalar multiplication and
vector addition) and creates a single brand-new vector, of the same
size as the original vectors.  When a definition or theorem employs a
linear combination, think about the nature of the objects that go into
its creation (lists of scalars and vectors), and the type of object
that results (a single vector).

\begin{example}[Two linear combinations in $\complex{6}$]
  Suppose that
  \begin{align*}
    \alpha_1=1&&\alpha_2=-4&&\alpha_3=2&&\alpha_4=-1
  \end{align*}
  and
  \begin{align*}
    \vect{u}_1&=\colvector{2\\4\\-3\\1\\2\\9}&
    \vect{u}_2&=\colvector{6\\3\\0\\-2\\1\\4}&
    \vect{u}_3&=\colvector{-5\\2\\1\\1\\-3\\0}&
    \vect{u}_4&=\colvector{3\\2\\-5\\7\\1\\3}
  \end{align*}
  then their linear combination is
  \begin{align*}
    \alpha_1\vect{u_1}+ \alpha_2\vect{u_2}+ \alpha_3\vect{u_3}+ \alpha_4\vect{u_4}&=
    (1)\colvector{2\\4\\-3\\1\\2\\9}+
    (-4)\colvector{6\\3\\0\\-2\\1\\4}+
    (2)\colvector{-5\\2\\1\\1\\-3\\0}+
    (-1)\colvector{3\\2\\-5\\7\\1\\3}\\
    &=
    \colvector{2\\4\\-3\\1\\2\\9}+
    \colvector{-24\\-12\\0\\8\\-4\\-16}+
    \colvector{-10\\\answer{4}\\2\\2\\-6\\0}+
    \colvector{-3\\\answer{-2}\\5\\-7\\-1\\-3} \\
    &=\colvector{-35\\\answer{-6}\\4\\4\\-9\\-10}
  \end{align*}
  
  A different linear combination, of the same set of vectors, can be
  formed with different scalars. Take
  \begin{align*}
    \beta_1=3&&\beta_2=0&&\beta_3=5&&\beta_4=-1
  \end{align*}
  and form the linear combination
  \begin{align*}
    \beta_1\vect{u_1}+ \beta_2\vect{u_2}+ \beta_3\vect{u_3}+ \beta_4\vect{u_4}
    &=
      (3)\colvector{2\\4\\-3\\1\\2\\9}+
      (0)\colvector{6\\3\\0\\-2\\1\\4}+
      (5)\colvector{-5\\2\\1\\1\\-3\\0}+
      (-1)\colvector{3\\2\\-5\\7\\1\\3}\\
    &=
      \colvector{6\\12\\-9\\3\\6\\27}+
      \colvector{0\\0\\0\\0\\0\\0}+
      \colvector{-25\\\answer{10}\\5\\5\\-15\\0}+
      \colvector{-3\\\answer{-2}\\5\\-7\\-1\\-3} \\
    &=
      \colvector{-22\\\answer{20}\\1\\1\\-10\\24}
  \end{align*}
  Notice how we could keep our set of vectors fixed, and use different
  sets of scalars to construct different vectors.  

  \begin{question}
    You might build a
    few new linear combinations of
    $\vect{u}_1,\,\vect{u}_2,\,\vect{u}_3,\,\vect{u}_4$ right now.  We
    will be right here when you get back.  What vectors were you able to
    create?  Do you think you could create the vector 
    \[\vect{w}=\colvector{13\\15\\5\\-17\\2\\25}\]
    ``suitable'' choice of four scalars?
  
    \begin{multipleChoice}
      \choice{Yes.}
      \choice[correct]{No.}
    \end{multipleChoice}

    \begin{feedback}[correct]
      No, it is not possible to create $\vect{w}$ as a linear
      combination of the four vectors
      $\vect{u}_1,\,\vect{u}_2,\,\vect{u}_3,\,\vect{u}_4$.  By
      creating the desired linear combination with unknowns as
      scalars, \ref{theorem:SLSLC} provides a system of equations that
      has no solution.  This one computation is enough to show us that
      it is not possible to create all the vectors of $\complex{6}$
      through linear combinations of the four vectors
      $\vect{u}_1,\,\vect{u}_2,\,\vect{u}_3,\,\vect{u}_4$.

      So not every vector in $\complex{6}$ can be expressed as a
      linear combination of
      $\vect{u}_1,\,\vect{u}_2,\,\vect{u}_3,\,\vect{u}_4$.  But if we
      chose four other vectors
      $\vect{b}_1,\,\vect{b}_2,\,\vect{b}_3,\,\vect{b}_4$, could it be
      possible that \textit{every} vector from $\complex{6}$ can be
      expressed as a linear combination of those four vectors
      $\vect{b}_1,\,\vect{b}_2,\,\vect{b}_3,\,\vect{b}_4$?  Questions
      like this one are fundamental, and time spent considering them
      \textit{now} will prove beneficial later.
    \end{feedback}
  \end{question}

\end{example}

\begin{example}
  The system of $m=\answer{3}$ linear equations
  \begin{align*}
    -7x_1 -6 x_2 - 12x_3 &=-33\\
    5x_1  + 5x_2 + 7x_3 &=24\\
    x_1 +4x_3 &=5
  \end{align*}
  can be rewritten as the vector equality
  \[
    \colvector{\answer{-7}x_1 -6 x_2 - 12x_3\\ 5x_1  + 5x_2 + 7x_3\\ x_1 +4x_3}
    =
    \colvector{-33\\\answer{24}\\5}
  \]
  Now we will break up the linear expressions on the left, first using vector addition,
  \[
    \colvector{-7x_1\\ 5x_1\\x_1}+
    \colvector{-6 x_2\\5x_2\\0x_2}+
    \colvector{-12x_3\\7x_3\\4x_3}
    =
    \colvector{-33\\24\\5}
  \]
  Now we can rewrite each of these vectors as a scalar multiple of a
  fixed vector, where the scalar is one of the unknown variables,
  converting the left-hand side into a linear combination
  \[
    x_1\colvector{\answer{-7}\\5\\1}+
    x_2\colvector{-6\\5\\0}+
    x_3\colvector{-12\\7\\4}
    =
    \colvector{-33\\24\\5}
  \]
  
  \begin{question}
    What is the solution to this system?

    We can now interpret the problem of solving the system of
    equations as determining values for the scalar multiples that make
    the vector equation true.  We can determine that this system has
    only one solution: a quick way to see this is to row-reduce the
    coefficient matrix to the $3\times 3$ identity matrix and apply
    \ref{theorem:NMRRI} to determine that the coefficient matrix is
    nonsingular.  Then \ref{theorem:NMUS} tells us that the system of
    equations has a unique solution.  This solution is
    \begin{align*}
      x_1 = -3&&x_2 = 5&&x_3 = \answer{2}
    \end{align*}

    \begin{feedback}[correct]
      So, in the context of this example, we can express the fact that
      these values of the variables are a solution by writing the
      linear combination,
      \[
        (-3)\colvector{-7\\5\\1}+
        (5)\colvector{-6\\5\\0}+
        (2)\colvector{-12\\7\\4}
        =
        \colvector{-33\\24\\5}
      \]
      Furthermore, these are the \textit{only} three scalars that will
      accomplish this equality, since they come from a unique
      solution.

      Notice how the three vectors in this example are the columns of
      the coefficient matrix of the system of equations.  This is our
      first hint of the important interplay between the vectors that
      form the columns of a matrix, and the matrix itself.
    \end{feedback}
  \end{question}

\end{example}

\begin{example}
  As a vector equality, the system of linear equations
  \begin{align*}
    x_1 -x_2 +2x_3 & =1\\
    2x_1+ x_2 + x_3 & =8\\
    x_1 + x_2 & =5
  \end{align*}            
  can be written as
  \[
    \colvector{x_1 -x_2 +2x_3\\2x_1+ x_2 + x_3\\ x_1 + x_2}
    =
    \colvector{1\\8\\5}
  \]
  Now break up the linear expressions on the left, first using vector addition,
  \[
    \colvector{x_1\\2x_1\\x_1}+
    \colvector{-x_2\\x_2\\x_2}+
    \colvector{2x_3\\x_3\\0x_3}
    =
    \colvector{1\\\answer{8}\\5}
  \]
  Rewrite each of these vectors as a scalar multiple of a fixed
  vector, where the scalar is one of the unknown variables, converting
  the left-hand side into a linear combination
  \[
    x_1\colvector{1\\2\\1}+
    x_2\colvector{-1\\1\\1}+
    x_3\colvector{\answer{2}\\1\\0}
    =
    \colvector{1\\8\\5}
  \]
  Row-reducing the corresponding augmented matrix leads to the
  conclusion that the system is consistent and has free variables,
  hence infinitely many solutions.  So for example, the two solutions
  \begin{align*}
    x_1 = 2&&x_2 = 3&&x_3 = 1\\
    x_1 = 3&&x_2 = 2&&x_3 = 0
  \end{align*}
  can be used together to say that,
  \[
    (2)\colvector{1\\2\\1}+
    (3)\colvector{-1\\1\\1}+
    (1)\colvector{2\\1\\0}
    =
    \colvector{1\\8\\5}
    =
    (3)\colvector{1\\2\\1}+
    (2)\colvector{-1\\1\\1}+
    (0)\colvector{2\\1\\0}
  \]
  Ignoring the middle of this equation and moving all the terms to the left-hand side yields
  \[
    (2)\colvector{1\\2\\1}+
    (3)\colvector{-1\\1\\1}+
    (1)\colvector{2\\1\\0}+
    (-3)\colvector{1\\2\\1}+
    (-2)\colvector{-1\\1\\1}+
    (-0)\colvector{2\\1\\0}
    =
    \colvector{0\\0\\0}
  \]
  Regrouping gives
  \[
    (-1)\colvector{1\\2\\1}+
    (1)\colvector{-1\\1\\1}+
    (1)\colvector{2\\1\\0}
    =
    \colvector{0\\0\\0}
  \]
  Notice that these three vectors are the columns of the coefficient
  matrix for the original system of equations.  This equality says
  there is a linear combination of those columns that equals the
  vector of all zeros.  Give it some thought, but this says that
  \begin{align*}
    x_1=-1&&x_2=1&&x_3=1
  \end{align*}
  is a \wordChoice{\choice{trivial}\choice[correct]{nontrivial}}
  solution to the homogeneous system of equations with the coefficient
  matrix for the original system.  In particular, this demonstrates
  that this coefficient matrix is
  \wordChoice{\choice{nonsingular}\choice[correct]{singular}}.
\end{example}

There is a lot going on in the last two examples.  Come back to them
in a while and make some connections with the intervening material.
For now, we will summarize and explain some of this behavior with a
theorem.

\begin{theorem}[Solutions to Linear Systems are Linear Combinations]
  \label{theorem:SLSLC}
  Denote the columns of the $m\times n$ matrix $A$ as the vectors
  $\vectorlist{A}{n}$.  Then $\vect{x}\in\complex{n}$ is a solution to
  the linear system of equations $\linearsystem{A}{\vect{b}}$ if and
  only if $\vect{b}$ equals the linear combination of the columns of
  $A$ formed with the entries of $\vect{x}$,
  \[
    \vectorentry{\vect{x}}{1}\vect{A}_1+
    \vectorentry{\vect{x}}{2}\vect{A}_2+
    \vectorentry{\vect{x}}{3}\vect{A}_3+
    \cdots+
    \vectorentry{\vect{x}}{n}\vect{A}_n
    =
    \vect{b}
  \]

  \begin{proof}
    The proof of this theorem is as much about a change in notation as
    it is about making logical deductions.  Write the system of
    equations $\linearsystem{A}{\vect{b}}$ as
    \begin{align*}
      a_{11}x_1+a_{12}x_2+a_{13}x_3+\dots+a_{1n}x_n&=b_1\\
      a_{21}x_1+a_{22}x_2+a_{23}x_3+\dots+a_{2n}x_n&=b_2\\
      a_{31}x_1+a_{32}x_2+a_{33}x_3+\dots+a_{3n}x_n&=b_3\\
      \vdots&\\
      a_{m1}x_1+a_{m2}x_2+a_{m3}x_3+\dots+a_{mn}x_n&=b_m
    \end{align*}
    
    Notice then that the entry of the coefficient matrix $A$ in row $i$
    and column $j$ has two names: $a_{ij}$ as the coefficient of $x_j$
    in equation $i$ of the system and $\vectorentry{\vect{A}_j}{i}$ as
    the $i$-th entry of the column vector in column $j$ of the
    coefficient matrix $A$.  Likewise, entry $i$ of $\vect{b}$ has two
    names: $b_i$ from the linear system and $\vectorentry{\vect{b}}{i}$
    as an entry of a vector.  Our theorem is an equivalence so we need
    to prove both directions.
    
    ($\Leftarrow$) Suppose we have the vector equality between
    $\vect{b}$ and the linear combination of the columns of
    $\answer{A}$.  Then for $1\leq i\leq m$,
    \begin{align*}
      b_i
      &=\vectorentry{\vect{b}}{i}&&\ref{definition:CV}\\
      &=\vectorentry{
        \vectorentry{\vect{x}}{1}\vect{A}_1+
        \vectorentry{\vect{x}}{2}\vect{A}_2+
        \vectorentry{\vect{x}}{3}\vect{A}_3+
        \cdots+
        \vectorentry{\vect{x}}{n}\vect{A}_n
        }{i}&&\text{Hypothesis}\\
      &=
        \vectorentry{\vectorentry{\vect{x}}{1}\vect{A}_1}{i}+
        \vectorentry{\vectorentry{\vect{x}}{2}\vect{A}_2}{i}+
        \vectorentry{\vectorentry{\vect{x}}{3}\vect{A}_3}{i}+
        \cdots+
        \vectorentry{\vectorentry{\vect{x}}{n}\vect{A}_n}{i}
                                 &&\ref{definition:CVA}\\
      &=
        \vectorentry{\vect{x}}{1}\vectorentry{\vect{A}_1}{i}+
        \vectorentry{\vect{x}}{2}\vectorentry{\vect{A}_2}{i}+
        \vectorentry{\vect{x}}{3}\vectorentry{\vect{A}_3}{i}+
        \cdots+
        \vectorentry{\vect{x}}{n}\vectorentry{\vect{A}_n}{i}
                                 &&\ref{definition:CVSM}\\
      &=
        \vectorentry{\vect{x}}{1}a_{i1}+
        \vectorentry{\vect{x}}{2}a_{i2}+
        \vectorentry{\vect{x}}{3}a_{i3}+
        \cdots+
        \vectorentry{\vect{x}}{n}a_{in}
                                 &&\ref{definition:CV}\\
      &=
        a_{i1}\vectorentry{\vect{x}}{1}+
        a_{i2}\vectorentry{\vect{x}}{2}+
        a_{i3}\vectorentry{\vect{x}}{3}+
        \cdots+
        a_{in}\vectorentry{\vect{x}}{n}
                                 &&\ref{property:CMCN}
    \end{align*}
    This says that the entries of $\vect{x}$ form a solution to
    equation $\answer{i}$ of $\linearsystem{A}{\vect{b}}$ for all
    $1\leq i\leq m$, in other words, $\vect{x}$ is a solution to
    $\linearsystem{A}{\vect{b}}$.

    ($\Rightarrow$) Suppose now that $\vect{x}$ is a solution to the
    linear system $\linearsystem{A}{\vect{b}}$.  Then for all
    $1\leq i\leq m$,
    \begin{align*}
      \vectorentry{\vect{b}}{i}
      &=b_i&&\ref{definition:CV}\\
      &=
        a_{i1}\vectorentry{\vect{x}}{1}+
        a_{i2}\vectorentry{\vect{x}}{2}+
        a_{i3}\vectorentry{\vect{x}}{3}+
        \cdots+
        a_{in}\vectorentry{\vect{x}}{n}
           &&\text{Hypothesis}\\
      &=
        \vectorentry{\vect{x}}{1}a_{i1}+
        \vectorentry{\vect{x}}{2}a_{i2}+
        \vectorentry{\vect{x}}{3}a_{i3}+
        \cdots+
        \vectorentry{\vect{x}}{n}a_{in}
           &&\ref{property:CMCN}\\
      &=
        \vectorentry{\vect{x}}{1}\vectorentry{\vect{A}_1}{i}+
        \vectorentry{\vect{x}}{2}\vectorentry{\vect{A}_2}{i}+
        \vectorentry{\vect{x}}{3}\vectorentry{\vect{A}_3}{i}+
        \cdots+
        \vectorentry{\vect{x}}{n}\vectorentry{\vect{A}_n}{i}
           &&\ref{definition:CV}\\
      &=
        \vectorentry{\vectorentry{\vect{x}}{1}\vect{A}_1}{i}+
        \vectorentry{\vectorentry{\vect{x}}{2}\vect{A}_2}{i}+
        \vectorentry{\vectorentry{\vect{x}}{3}\vect{A}_3}{i}+
        \cdots+
        \vectorentry{\vectorentry{\vect{x}}{n}\vect{A}_n}{i}
           &&\ref{definition:CVSM}\\
      &=\vectorentry{
        \vectorentry{\vect{x}}{1}\vect{A}_1+
        \vectorentry{\vect{x}}{2}\vect{A}_2+
        \vectorentry{\vect{x}}{3}\vect{A}_3+
        \cdots+
        \vectorentry{\vect{x}}{n}\vect{A}_n
        }{i}&&\ref{definition:CVA}
    \end{align*}
    So the entries of the vector $\vect{b}$, and the entries of the
    vector that is the linear combination of the columns of
    $\answer{A}$, agree for all $1\leq i\leq m$.  By
    \ref{definition:CVE} we see that the two vectors are equal, as
    desired.
\end{proof}
\end{theorem}

\begin{example}
  In other words, this theorem tells us that solutions to systems of
  equations are linear combinations of the $n$ column vectors of the
  coefficient matrix ($\vect{A}_j$) which yield the constant vector
  $\vect{b}$.  Or said another way, a solution to a system of
  equations $\linearsystem{A}{\vect{b}}$ is an answer to the question
  ``How can I form the vector $\vect{b}$ as a linear combination of
  the \wordChoice{\choice{rows}\choice[correct]{columns}} of $A$?''
\end{example}

\end{document}

