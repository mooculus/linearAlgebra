\documentclass{ximera}

\input{../../preamble.tex}

\title{Computing the Inverse of a Matrix}

\begin{document}
\begin{abstract}
  There are computational procedures for computing the inverse of a
  matrix, and for determining when a matrix is and is not invertible.
\end{abstract}
\maketitle

We have seen inverses to matrices, but these inverse matrices have
just dropped from the sky.  How would we compute an inverse?  And just
when is a matrix invertible, and when is it not?  Writing a putative
inverse with $n^2$ unknowns and solving the resultant $n^2$ equations
is one approach.

Applying this approach to $2\times 2$ matrices can get us somewhere,
so just for fun, let us do it.

\begin{theorem}[Two-by-Two Matrix Inverse]
\label{theorem:TTMI}

Suppose
\[
  A=
  \begin{bmatrix}
    a&b\\
    c&d
  \end{bmatrix}
\]

Then $A$ is invertible if and only if $ad-bc\neq 0$.  When $A$ is invertible, then
\[
  \inverse{A}=\frac{1}{ad-bc}
  \begin{bmatrix}
    d&-b\\
    -c&a
  \end{bmatrix}
\]

\begin{proof}
  ($\Leftarrow$) Assume that $ad-bc\neq \answer{0}$.  We will use the
  definition of the inverse of a matrix to establish that $A$ has an
  inverse.  (Note that if $ad-bc\neq 0$ then the displayed formula for
  $\inverse{A}$ is legitimate since we are not dividing by zero).
  Using this proposed formula for the inverse of $A$, we compute
  \begin{align*}
    A\inverse{A}
    &=
      \begin{bmatrix}a&b\\c&d\end{bmatrix}
                             \left(\frac{1}{\answer{ad-bc}}
                             \begin{bmatrix}d&-b\\-c&a\end{bmatrix}
                                                      \right) \\
    &=
      \frac{1}{ad-bc}
      \begin{bmatrix}\answer{ad-bc}&0\\0&ad-bc\end{bmatrix} \\
    &= \begin{bmatrix}\answer{1}&0\\0&1\end{bmatrix}\\
  \end{align*}
  and
  \begin{align*}
    \inverse{A}A
    &=
      \frac{1}{ad-bc}
      \begin{bmatrix}d&-b\\-c&a\end{bmatrix}
                               \begin{bmatrix}a&b\\c&d\end{bmatrix} \\
                                                      &=
                                                      \frac{1}{ad-bc}
\begin{bmatrix}ad-bc&0\\0&ad-bc\end{bmatrix} \\
    &=
      \begin{bmatrix}1&0\\0&1\end{bmatrix}
  \end{align*}

  This is sufficient to establish that $A$ is invertible, and that the expression for $\inverse{A}$ is correct.
  
  ($\Rightarrow$) Assume that $A$ is invertible, and proceed with a
  proof by contradiction (\ref{technique:CD}), by assuming also that
  $ad-bc=0$.  This translates to $ad=bc$.  Let
  \[
    B=
    \begin{bmatrix}
      e&f\\
      g&h
    \end{bmatrix}
  \]
  be a putative inverse of $A$.
  
  This means that
  \[
    I_2=AB=
    \begin{bmatrix}
      a&b\\
      c&d
    \end{bmatrix}
    \begin{bmatrix}
      e&f\\
      g&h
    \end{bmatrix}
    =
    \begin{bmatrix}
      ae+bg & af+bh\\
      ce+dg & cf+dh
    \end{bmatrix}
  \]

  Working on the matrices on two ends of this equation, we will multiply the top row by $c$ and the bottom row by $a$.
  \[
    \begin{bmatrix}
      c&0\\
      0&a
    \end{bmatrix}
    =
    \begin{bmatrix}
      ace+bcg & acf+bch\\
      ace+adg & acf+adh
    \end{bmatrix}
  \]

  We are assuming that $ad=bc$, so we can replace two occurrences of $ad$ by $bc$ in the bottom row of the right matrix.
  \[
    \begin{bmatrix}
      c&0\\
      0&a
    \end{bmatrix}
    =
    \begin{bmatrix}
      ace+bcg & acf+bch\\
      ace+bcg & acf+bch
    \end{bmatrix}
  \]

  The matrix on the right now has two rows that are identical, and
  therefore the same must be true of the matrix on the left.
  Identical rows for the matrix on the left implies that $a=0$ and
  $c=0$.

  With this information, the product $AB$ becomes
  \[
    \begin{bmatrix}
      1 & 0\\
      0 & 1
    \end{bmatrix}
    =I_2
    =AB
    =
    \begin{bmatrix}
      ae+bg & af+bh\\
      ce+dg & cf+dh
    \end{bmatrix}
    =
    \begin{bmatrix}
      bg & bh\\
      dg & dh
    \end{bmatrix}
  \]
  
  So $bg=dh=1$ and thus $b,g,d,h$ are all nonzero.  But then $bh$ and
  $dg$ (the ``other corners'') must also be nonzero, so this is
  (finally) a contradiction.  So our assumption was false and we see
  that $ad-bc\neq 0$ whenever $A$ has an inverse.
\end{proof}
\end{theorem}

There are several ways one could try to prove this theorem, but there
is a continual temptation to divide by one of the eight entries
involved ($a$ through $f$), but we can never be sure if these numbers
are zero or not.  This could lead to an analysis by cases, which is
messy, messy, messy.  Note how the above proof never divides, but
always multiplies, and how zero/nonzero considerations are handled.
Pay attention to the expression $ad-bc$, as we will see it again when
we discuss \textit{determinants}.

This theorem is fun, and it is nice to have a formula for the inverse,
and a condition that tells us when we can use it.  However, this
approach becomes impractical for larger matrices, even though it is
possible to demonstrate that, in theory, there is a general formula.
(Think for a minute about extending this result to just $3\times 3$
matrices.  For starters, we need 18 letters!)  Instead, we will work
column-by-column.  Let us first work an example that will motivate the
main theorem and remove some of the previous mystery.

\begin{example}[Computing a matrix inverse]

Consider the matrix
\[
A=
\begin{bmatrix}
 1 & 2 & 1 & 2 & 1 \\
 -2 & -3 & 0 & -5 & -1 \\
 1 & 1 & 0 & 2 & 1 \\
 -2 & -3 & -1 & -3 & -2 \\
 -1 & -3 & -1 & -3 & 1
\end{bmatrix}
\]

For its inverse, we desire a matrix $B$ so that $AB=I_5$.  Emphasizing the structure of the columns and employing the definition of matrix multiplication \ref{definition:MM},
\begin{align*}    
AB&=I_5\\
A\lbrack \vect{B}_1|\vect{B}_2|\vect{B}_3|\vect{B}_4|\vect{B}_5]&=[\vect{e}_1|\vect{e}_2|\vect{e}_3|\vect{e}_4|\vect{e}_5\rbrack\\
\lbrack A\vect{B}_1|A\vect{B}_2|A\vect{B}_3|A\vect{B}_4|A\vect{B}_5]&=[\vect{e}_1|\vect{e}_2|\vect{e}_3|\vect{e}_4|\vect{e}_5\rbrack
\end{align*}

Equating the matrices column-by-column we have
\begin{align*}
A\vect{B}_1=\vect{e}_1&&
A\vect{B}_2=\vect{e}_2&&
A\vect{B}_3=\vect{e}_3&&
A\vect{B}_4=\vect{e}_4&&
A\vect{B}_5=\vect{e}_5.
\end{align*}


Since the matrix $B$ is what we are trying to compute, we can view
each column, $\vect{B}_i$, as a column vector of unknowns.  Then we
have five systems of equations to solve, each with 5 equations in 5
variables.  Notice that all 5 of these systems have the same
coefficient matrix.  We will now solve each system in turn.
Row-reducing the augmented matrix of the linear system $\linearsystem{A}{\vect{e}_1}$,
\begin{align*}
  \begin{bmatrix}
    1 & 2 & 1 & 2 & 1 & 1\\
    -2 & -3 & 0 & -5 & -1 & 0\\
    1 & 1 & 0 & 2 & 1 & 0\\
    -2 & -3 & -1 & -3 & -2 & 0\\
    -1 & -3 & -1 & -3 & 1 & 0
  \end{bmatrix}
                            \rref
                            \begin{bmatrix}
                              \leading{1} & 0 & 0 & 0 & 0 & -3\\
                              0 & \leading{1} & 0 & 0 & 0 & 0\\
                              0 & 0 & \leading{1} & 0 & 0 & 1\\
                              0 & 0 & 0 & \leading{1} & 0 & 1\\
                              0 & 0 & 0 & 0 & \leading{1} & 1
                            \end{bmatrix}
                                                            ;
                                                            \vect{B}_1=\colvector{-3\\0\\1\\1\\1}\\
\end{align*}
Row-reducing the augmented matrix of the linear system $\linearsystem{A}{\vect{e}_2}$,
\begin{align*}
\begin{bmatrix}
 1 & 2 & 1 & 2 & 1 & 0\\
 -2 & -3 & 0 & -5 & -1 & 1\\
 1 & 1 & 0 & 2 & 1 & 0\\
 -2 & -3 & -1 & -3 & -2 & 0\\
 -1 & -3 & -1 & -3 & 1 & 0
\end{bmatrix}
\rref
\begin{bmatrix}
\leading{1} & 0 & 0 & 0 & 0 & 3\\
0 & \leading{1} & 0 & 0 & 0 & -2\\
0 & 0 & \leading{1} & 0 & 0 & 2\\
0 & 0 & 0 & \leading{1} & 0 & 0\\
0 & 0 & 0 & 0 & \leading{1} & -1
\end{bmatrix}
;
\vect{B}_2=\colvector{3\\-2\\2\\0\\-1}\\
\end{align*}
Row-reduce the augmented matrix of the linear system $\linearsystem{A}{\vect{e}_3}$,
\begin{align*}
\begin{bmatrix}
 1 & 2 & 1 & 2 & 1 & 0\\
 -2 & -3 & 0 & -5 & -1 & 0\\
 1 & 1 & 0 & 2 & 1 & 1\\
 -2 & -3 & -1 & -3 & -2 & 0\\
 -1 & -3 & -1 & -3 & 1 & 0
\end{bmatrix}
\rref
\begin{bmatrix}
\leading{1} & 0 & 0 & 0 & 0 & 6\\
0 & \leading{1} & 0 & 0 & 0 & -5\\
0 & 0 & \leading{1} & 0 & 0 & 4\\
0 & 0 & 0 & \leading{1} & 0 & 1\\
0 & 0 & 0 & 0 & \leading{1} & -2
\end{bmatrix}
;
\vect{B}_3=\colvector{6\\-5\\4\\1\\-2}\\
\end{align*}
Row-reduce the augmented matrix of the linear system $\linearsystem{A}{\vect{e}_4}$,
\begin{align*}
\begin{bmatrix}
 1 & 2 & 1 & 2 & 1 & 0\\
 -2 & -3 & 0 & -5 & -1 & 0\\
 1 & 1 & 0 & 2 & 1 & 0\\
 -2 & -3 & -1 & -3 & -2 & 1\\
 -1 & -3 & -1 & -3 & 1 & 0
\end{bmatrix}
\rref
\begin{bmatrix}
\leading{1} & 0 & 0 & 0 & 0 & -1\\
0 & \leading{1} & 0 & 0 & 0 & -1\\
0 & 0 & \leading{1} & 0 & 0 & 1\\
0 & 0 & 0 & \leading{1} & 0 & 1\\
0 & 0 & 0 & 0 & \leading{1} & 0
\end{bmatrix}
;
\vect{B}_4=\colvector{-1\\-1\\1\\1\\0}\\
\end{align*}
And row-reducing the augmented matrix of the linear system $\linearsystem{A}{\vect{e}_5}$,
\begin{align*}
\begin{bmatrix}
 1 & 2 & 1 & 2 & 1 & 0\\
 -2 & -3 & 0 & -5 & -1 & 0\\
 1 & 1 & 0 & 2 & 1 & 0\\
 -2 & -3 & -1 & -3 & -2 & 0\\
 -1 & -3 & -1 & -3 & 1 & 1
\end{bmatrix}
\rref
\begin{bmatrix}
\leading{1} & 0 & 0 & 0 & 0 & -2\\
0 & \leading{1} & 0 & 0 & 0 & 1\\
0 & 0 & \leading{1} & 0 & 0 & -1\\
0 & 0 & 0 & \leading{1} & 0 & 0\\
0 & 0 & 0 & 0 & \leading{1} & 1
\end{bmatrix}
;
\vect{B}_5=\colvector{-2\\1\\-1\\0\\1}\\
\end{align*}
We can now collect our 5 solution vectors into the matrix $B$,
\begin{align*}
B=
&[\vect{B}_1|\vect{B}_2|\vect{B}_3|\vect{B}_4|\vect{B}_5]\\
=&
\left[\colvector{-3\\0\\1\\1\\1}
\left\lvert\colvector{3\\-2\\2\\0\\-1}\right.
\left\lvert\colvector{6\\-5\\4\\1\\-2}\right.
\left\lvert\colvector{-1\\-1\\1\\1\\0}\right.
\left\lvert\colvector{-2\\1\\-1\\0\\1}\right.
\right]\\
&=
\begin{bmatrix}
 -3 & 3 & 6 & -1 & -2 \\
 0 & -2 & -5 & -1 & 1 \\
 1 & 2 & 4 & 1 & -1 \\
 1 & 0 & 1 & 1 & 0 \\
 1 & -1 & -2 & 0 & 1
\end{bmatrix}
\end{align*}

By this method, we know that $AB=I_5$.  Check that $BA=I_5$, and then we will know that we have the inverse of $A$.
\end{example}

Notice how the five systems of equations in the preceding example were
all solved by \textit{exactly} the same sequence of row operations.
Would it not be nice to avoid this obvious duplication of effort?  Our
main theorem for this section follows, and it mimics this previous
example, while also avoiding all the overhead.

\begin{theorem}[Computing the Inverse of a Nonsingular Matrix]
\label{theorem:CINM}

Suppose $A$ is a nonsingular square matrix of size $n$.  Create the
$n\times 2n$ matrix $M$ by placing the $n\times n$ identity matrix
$I_n$ to the right of the matrix $A$.  Let $N$ be a matrix that is
row-equivalent to $M$ and in reduced row-echelon form.  Finally, let
$J$ be the matrix formed from the final $n$ columns of $N$. Then
$AJ=I_n$.

\begin{proof}
  $A$ is nonsingular, so there is a sequence of row operations that
  will convert $A$ into $I_n$.  It is this same sequence of row
  operations that will convert $M$ into $N$, since having the identity
  matrix in the first $n$ columns of $N$ is sufficient to guarantee
  that $N$ is in reduced row-echelon form.

  If we consider the systems of linear equations,
  $\linearsystem{A}{\vect{e}_i}$, $1\leq i\leq n$, we see that the
  aforementioned sequence of row operations will also bring the
  augmented matrix of each of these systems into reduced row-echelon
  form.  Furthermore, the unique solution to
  $\linearsystem{A}{\vect{e}_i}$ appears in column $n+1$ of the
  row-reduced augmented matrix of the system and is identical to
  column $n+i$ of $N$.  Let $\vectorlist{N}{2n}$ denote the columns of
  $N$.  So we find,
  \begin{align*}
    AJ=&A[\vect{N}_{n+1}|\vect{N}_{n+2}|\vect{N}_{n+3}|\ldots|\vect{N}_{n+n}]\\
    =&[A\vect{N}_{n+1}|A\vect{N}_{n+2}|A\vect{N}_{n+3}|\ldots|A\vect{N}_{n+n}]\\
    =&[\vect{e}_1|\vect{e}_2|\vect{e}_3|\ldots|\vect{e}_n]\\
    =&I_n
  \end{align*}
  as desired.

\end{proof}
\end{theorem}

We have to be just a bit careful here about both what this theorem
says and what it does not say.  If $A$ is a nonsingular matrix, then
we are guaranteed a matrix $B$ such that $AB=I_n$, and the proof gives
us a process for constructing $B$.  However, the definition of the
inverse of a matrix requires that $BA=I_n$ also.  So at this juncture
we must compute the matrix product in the ``opposite'' order before we
claim $B$ as the inverse of $A$.  However, we will soon see that this
is \textit{always} the case, in \ref{theorem:OSIS}, so the title of
this theorem is not inaccurate.

What if $A$ is singular?  At this point we only know that
\ref{theorem:CINM} cannot be applied.  The question of $A$'s inverse
is still open.  (But see \ref{theorem:NI} in the next section.)

We will finish by computing the inverse in another example.

\begin{example}[Computing a matrix inverse]
  Consider the matrix
  \begin{align*}
    B=&\begin{bmatrix}
      -7&-6&-12\\
      5&5&7\\
      1&0&4
    \end{bmatrix}
  \end{align*}
  Exercising \ref{theorem:CINM} we set
  \begin{align*}
    M=&
        \begin{bmatrix}
          -7&-6&-12&1&0&0\\
          5&5&7&0&1&0\\
          1&0&4&0&0&1
        \end{bmatrix}
  \end{align*}
  which row reduces to
  \begin{align*}
    N=&
        \begin{bmatrix}
          1&0&0&\answer{-10} & -12 & -9\\
          0&1&0&\frac{13}{2} & 8 & \frac{11}{2}\\
          0&0&1&\frac{5}{2} & 3 & \frac{5}{2}
        \end{bmatrix}.\\
  \end{align*}
  And so
  \begin{align*}
    \inverse{B}=&
                  \begin{bmatrix}
                    -10 & -12 & -9\\
                    \frac{13}{2} & 8 & \frac{11}{2}\\
                    \frac{5}{2} & 3 & \frac{5}{2}
                  \end{bmatrix}
  \end{align*}
  once we check that $\inverse{B}B=I_3$ (the product in the opposite order is a consequence of the theorem).
\end{example}

\end{document}

